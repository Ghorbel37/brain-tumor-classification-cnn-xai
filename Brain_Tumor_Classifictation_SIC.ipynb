{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu7RQiAeH1Zs"
      },
      "source": [
        "# Install dependencies and preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsaehCHvolce"
      },
      "source": [
        "## Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRswrTGaogUi"
      },
      "outputs": [],
      "source": [
        "# Step 1. Setup & Install Dependencies\n",
        "!pip install -q tensorflow keras opencv-python matplotlib scikit-learn kagglehub tf-explain lime seaborn scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUMdvvSBo9xC"
      },
      "source": [
        "## Drive Mount & Directory Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPjW9_qtpYCV"
      },
      "outputs": [],
      "source": [
        "#ignore this part it's for access to my drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "    except:\n",
        "        pass\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    DRIVE_AVAILABLE = True\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Google Drive mounting failed: {e}\")\n",
        "    print(\"üìÅ Will save outputs locally to /content/ instead\")\n",
        "    DRIVE_AVAILABLE = False\n",
        "\n",
        "if DRIVE_AVAILABLE:\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroScan'\n",
        "else:\n",
        "    OUTPUT_DIR = '/content/NeuroScan'\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\" Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6d-NnZG04x_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bF9kpVD1mQO"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Best_Models_SIC/best_modelVGG16.keras\" \"/content/best_modelVGG16.keras\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0lc-CjBo-ZC"
      },
      "source": [
        "## Import Libraries & Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qryyVsbTpy_o"
      },
      "outputs": [],
      "source": [
        "# Dataset Download & Exploration (Phase 1)\n",
        "import kagglehub\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Download dataset\n",
        "print(\"‚¨áÔ∏è Downloading dataset...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download('shuvokumarbasakbd/brain-tumors-mri-crystal-clean-colorized-mri-data')\n",
        "    print('‚úÖ Dataset downloaded to:', path)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Dataset download failed: {e}\")\n",
        "    print(\"üí° Make sure you have internet connection and Kaggle credentials configured\")\n",
        "    raise\n",
        "\n",
        "# Dataset exploration\n",
        "train_path = os.path.join(path, 'dataset', 'train')\n",
        "test_path = os.path.join(path, 'dataset', 'test')\n",
        "validation_path = os.path.join(path, 'dataset', 'validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5DeHQb1o-Si"
      },
      "source": [
        "## Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SglJukaXqfRQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä DATASET STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "#Count images per class\n",
        "train_stats = {}\n",
        "test_stats = {}\n",
        "validation_stats = {}\n",
        "\n",
        "for class_name in os.listdir(train_path):\n",
        "    class_path = os.path.join(train_path, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        train_stats[class_name] = count\n",
        "\n",
        "for class_name in os.listdir(test_path):\n",
        "    class_path = os.path.join(test_path, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        test_stats[class_name] = count\n",
        "\n",
        "if os.path.exists(validation_path):\n",
        "    for class_name in os.listdir(validation_path):\n",
        "        class_path = os.path.join(validation_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "            validation_stats[class_name] = count\n",
        "\n",
        "print(f\"\\nüìÅ Training Set:\")\n",
        "for cls, count in sorted(train_stats.items()):\n",
        "    print(f\"  {cls}: {count:,} images\")\n",
        "print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"  Total: {sum(train_stats.values()):,} images\")\n",
        "\n",
        "if validation_stats:\n",
        "    print(f\"\\nüìÅ Validation Set:\")\n",
        "    for cls, count in sorted(validation_stats.items()):\n",
        "        print(f\"  {cls}: {count:,} images\")\n",
        "    print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(f\"  Total: {sum(validation_stats.values()):,} images\")\n",
        "\n",
        "print(f\"\\nüìÅ Testing Set:\")\n",
        "for cls, count in sorted(test_stats.items()):\n",
        "    print(f\"  {cls}: {count:,} images\")\n",
        "print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"  Total: {sum(test_stats.values()):,} images\")\n",
        "\n",
        "# Check class balance\n",
        "print(\"\\nüìä Class Distribution Analysis:\")\n",
        "total_train = sum(train_stats.values())\n",
        "for cls, count in sorted(train_stats.items()):\n",
        "    percentage = (count / total_train) * 100\n",
        "    bar = '‚ñà' * int(percentage / 2)\n",
        "    print(f\"  {cls:20s}: {percentage:5.2f}% {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzs_DJClqvBe"
      },
      "source": [
        "## Visualize Dataset Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2IzjUO3rF9X"
      },
      "outputs": [],
      "source": [
        "# Visualize dataset distribution\n",
        "num_plots = 3 if validation_stats else 2\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(7*num_plots, 6))   # Increased height\n",
        "\n",
        "if num_plots == 2:\n",
        "    axes = [axes[0], axes[1]]\n",
        "\n",
        "# Get classes and counts\n",
        "classes = sorted(train_stats.keys())\n",
        "train_counts = [train_stats[c] for c in classes]\n",
        "test_counts = [test_stats[c] for c in classes]\n",
        "\n",
        "# Dynamic offset function\n",
        "def label_offset(values):\n",
        "    return max(values) * 0.03\n",
        "\n",
        "# --- TRAINING SET PLOT ---\n",
        "axes[0].bar(classes, train_counts, color='skyblue', edgecolor='navy', linewidth=1.5)\n",
        "axes[0].set_title('Training Set Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Images', fontsize=11)\n",
        "axes[0].set_xlabel('Class', fontsize=11)\n",
        "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "for i, v in enumerate(train_counts):\n",
        "    axes[0].text(i, v + label_offset(train_counts), str(v),\n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# --- VALIDATION + TEST OR JUST TEST ---\n",
        "if validation_stats:\n",
        "\n",
        "    validation_counts = [validation_stats[c] for c in classes]\n",
        "\n",
        "    # VALIDATION\n",
        "    axes[1].bar(classes, validation_counts, color='lightgreen',\n",
        "                edgecolor='darkgreen', linewidth=1.5)\n",
        "    axes[1].set_title('Validation Set Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Number of Images', fontsize=11)\n",
        "    axes[1].set_xlabel('Class', fontsize=11)\n",
        "    axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for i, v in enumerate(validation_counts):\n",
        "        axes[1].text(i, v + label_offset(validation_counts), str(v),\n",
        "                     ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # TEST\n",
        "    axes[2].bar(classes, test_counts, color='lightcoral',\n",
        "                edgecolor='darkred', linewidth=1.5)\n",
        "    axes[2].set_title('Testing Set Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_ylabel('Number of Images', fontsize=11)\n",
        "    axes[2].set_xlabel('Class', fontsize=11)\n",
        "    axes[2].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for i, v in enumerate(test_counts):\n",
        "        axes[2].text(i, v + label_offset(test_counts), str(v),\n",
        "                     ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "else:\n",
        "    # TEST ONLY (no validation)\n",
        "    axes[1].bar(classes, test_counts, color='lightcoral',\n",
        "                edgecolor='darkred', linewidth=1.5)\n",
        "    axes[1].set_title('Testing Set Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Number of Images', fontsize=11)\n",
        "    axes[1].set_xlabel('Class', fontsize=11)\n",
        "    axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for i, v in enumerate(test_counts):\n",
        "        axes[1].text(i, v + label_offset(test_counts), str(v),\n",
        "                     ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Adjust layout to prevent collisions\n",
        "plt.subplots_adjust(top=0.88)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{OUTPUT_DIR}/dataset_distribution.png',\n",
        "            dpi=300, bbox_inches='tight')\n",
        "\n",
        "print(f\"\\nüíæ Saved: {OUTPUT_DIR}/dataset_distribution.png\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBuWsUuqqch"
      },
      "source": [
        "## Sample Images Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtq9ohqGrqgW"
      },
      "outputs": [],
      "source": [
        "# Sample visualization\n",
        "print(\"\\nüñºÔ∏è Sample Images from Dataset:\")\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "img_idx = 0\n",
        "for class_name in sorted(train_stats.keys()):\n",
        "    class_path = os.path.join(train_path, class_name)\n",
        "    sample_imgs = [os.path.join(class_path, f) for f in os.listdir(class_path)[:2]\n",
        "                   if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    for img_path in sample_imgs:\n",
        "        if img_idx < len(axes):\n",
        "            img = plt.imread(img_path)\n",
        "            axes[img_idx].imshow(img)\n",
        "            axes[img_idx].set_title(f'{class_name}', fontsize=11, fontweight='bold')\n",
        "            axes[img_idx].axis('off')\n",
        "            img_idx += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/sample_images.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"üíæ Saved: {OUTPUT_DIR}/sample_images.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Phase 1 Complete: Dataset downloaded and explored\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSf93i9jqqXM"
      },
      "source": [
        "## Import Preprocessing Libraries & Defining preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVKaRCyZr-HJ"
      },
      "outputs": [],
      "source": [
        "# Step 3. Data Preprocessing (Phase 2)\n",
        "import cv2\n",
        "from skimage import exposure\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî¨ DATA PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def preprocess_mri_image(img_path):\n",
        "    \"\"\"\n",
        "    MRI preprocessing:\n",
        "    1. Load and convert to grayscale\n",
        "    2. Skull stripping (brain extraction)\n",
        "    3. Noise reduction\n",
        "    4. Contrast enhancement (CLAHE)\n",
        "    5. Normalization\n",
        "    6. Resize to standard dimensions\n",
        "    7. Convert to RGB (3 channels as per action plan)\n",
        "\n",
        "    Returns: RGB image (224, 224, 3) normalized to [0, 1]\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load: {img_path}\")\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Skull stripping using Otsu's thresholding\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    _, brain_mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    brain_mask = cv2.morphologyEx(brain_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "    brain_mask = cv2.morphologyEx(brain_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "\n",
        "    # Apply mask\n",
        "    brain_only = cv2.bitwise_and(gray, gray, mask=brain_mask)\n",
        "\n",
        "    # Noise reduction\n",
        "    denoised = cv2.fastNlMeansDenoising(brain_only, None, h=10,\n",
        "                                        templateWindowSize=7, searchWindowSize=21)\n",
        "\n",
        "    # CLAHE for contrast enhancement\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced = clahe.apply(denoised)\n",
        "\n",
        "    # Normalization (Z-score on non-zero pixels)\n",
        "    non_zero = enhanced[enhanced > 0]\n",
        "    if len(non_zero) > 0:\n",
        "        mean_val = np.mean(non_zero)\n",
        "        std_val = np.std(non_zero)\n",
        "        if std_val > 0:\n",
        "            normalized = np.where(enhanced > 0, (enhanced - mean_val) / std_val, 0)\n",
        "            normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + 1e-8)\n",
        "        else:\n",
        "            normalized = enhanced / 255.0\n",
        "    else:\n",
        "        normalized = enhanced / 255.0\n",
        "\n",
        "    # Resize to target dimensions\n",
        "    resized = cv2.resize(normalized, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Convert to RGB (3 channels like mentioned in the action plan)\n",
        "    rgb = cv2.cvtColor((resized * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
        "    rgb = rgb.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert RGB to BGR before returning\n",
        "    bgr = cv2.cvtColor((rgb * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "    bgr = bgr.astype(np.float32) / 255.0\n",
        "\n",
        "    return bgr  # Returns BGR format (what cv2.imwrite expects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF8ufTSSsEVN"
      },
      "source": [
        "## Preprocessing Visualization Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o2Vf18LsRir"
      },
      "outputs": [],
      "source": [
        "# Visualize preprocessing steps\n",
        "def show_preprocessing_steps(img_path):\n",
        "    \"\"\"Visualize each preprocessing step\"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    _, mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "    brain_only = cv2.bitwise_and(gray, gray, mask=mask)\n",
        "    denoised = cv2.fastNlMeansDenoising(brain_only, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced = clahe.apply(denoised)\n",
        "    final = preprocess_mri_image(img_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
        "\n",
        "    axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0, 0].set_title('1. Original', fontweight='bold', fontsize=12)\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "    axes[0, 1].imshow(gray, cmap='gray')\n",
        "    axes[0, 1].set_title('2. Grayscale', fontweight='bold', fontsize=12)\n",
        "    axes[0, 1].axis('off')\n",
        "\n",
        "    axes[0, 2].imshow(mask, cmap='gray')\n",
        "    axes[0, 2].set_title('3. Brain Mask', fontweight='bold', fontsize=12)\n",
        "    axes[0, 2].axis('off')\n",
        "\n",
        "    axes[0, 3].imshow(brain_only, cmap='gray')\n",
        "    axes[0, 3].set_title('4. Brain Extracted', fontweight='bold', fontsize=12)\n",
        "    axes[0, 3].axis('off')\n",
        "\n",
        "    axes[1, 0].imshow(denoised, cmap='gray')\n",
        "    axes[1, 0].set_title('5. Denoised', fontweight='bold', fontsize=12)\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "    axes[1, 1].imshow(enhanced, cmap='gray')\n",
        "    axes[1, 1].set_title('6. CLAHE Enhanced', fontweight='bold', fontsize=12)\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    axes[1, 2].imshow(cv2.cvtColor((final * 255).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
        "    axes[1, 2].set_title('7. Final Preprocessed (RGB)', fontweight='bold', fontsize=12)\n",
        "    axes[1, 2].axis('off')\n",
        "\n",
        "    axes[1, 3].hist(gray.ravel(), bins=50, alpha=0.6, label='Original', color='blue', edgecolor='navy')\n",
        "    axes[1, 3].hist(enhanced.ravel(), bins=50, alpha=0.6, label='Enhanced', color='red', edgecolor='darkred')\n",
        "    axes[1, 3].set_title('8. Histogram Comparison', fontweight='bold', fontsize=12)\n",
        "    axes[1, 3].set_xlabel('Pixel Intensity')\n",
        "    axes[1, 3].set_ylabel('Frequency')\n",
        "    axes[1, 3].legend()\n",
        "    axes[1, 3].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiqKwFdxsEP5"
      },
      "source": [
        "## Show Sample Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a9o9AWDtIWh"
      },
      "outputs": [],
      "source": [
        "# Show preprocessing on samples from each class\n",
        "print(\"\\nüîç Visualizing preprocessing pipeline...\")\n",
        "for class_name in sorted(train_stats.keys())[:2]:\n",
        "    class_path = os.path.join(train_path, class_name)\n",
        "    sample_file = [os.path.join(class_path, f) for f in os.listdir(class_path)\n",
        "                   if f.endswith(('.jpg', '.png', '.jpeg'))][0]\n",
        "\n",
        "    print(f\"\\nüì∏ Processing sample from: {class_name}\")\n",
        "    fig = show_preprocessing_steps(sample_file)\n",
        "    save_path = f'{OUTPUT_DIR}/preprocessing_{class_name}.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"üíæ Saved: {save_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YHVT1hs4Qg"
      },
      "source": [
        "## Dataset Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H-5IbDZtaxH"
      },
      "outputs": [],
      "source": [
        "# Preprocess entire dataset\n",
        "def preprocess_dataset(input_dir, output_dir):\n",
        "    \"\"\"Preprocess and save entire dataset\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    total_processed = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    for class_name in os.listdir(input_dir):\n",
        "        class_path = os.path.join(input_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        output_class = os.path.join(output_dir, class_name)\n",
        "        os.makedirs(output_class, exist_ok=True)\n",
        "\n",
        "        files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "        print(f\"\\nüîÑ Processing {class_name}: {len(files)} images...\")\n",
        "        for img_file in tqdm(files, desc=class_name):\n",
        "            try:\n",
        "                img_path = os.path.join(class_path, img_file)\n",
        "                preprocessed = preprocess_mri_image(img_path)  # Returns BGR\n",
        "                output_path = os.path.join(output_class, img_file)\n",
        "                cv2.imwrite(output_path, (preprocessed * 255).astype(np.uint8))\n",
        "                total_processed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ö†Ô∏è Error processing {img_file}: {e}\")\n",
        "                total_errors += 1\n",
        "\n",
        "    return total_processed, total_errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oDkP0IEs4Lz"
      },
      "source": [
        "## Running Full Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk8JRL1kt7sx"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING FULL DATASET PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save to writable directory instead of read-only dataset path\n",
        "prep_train = os.path.join(OUTPUT_DIR, 'Preprocessed_Training')\n",
        "prep_test = os.path.join(OUTPUT_DIR, 'Preprocessed_Testing')\n",
        "prep_validation = os.path.join(OUTPUT_DIR, 'Preprocessed_Validation')\n",
        "\n",
        "print(\"\\nüìÇ Preprocessing Training Set...\")\n",
        "train_processed, train_errors = preprocess_dataset(train_path, prep_train)\n",
        "\n",
        "# Preprocess Validation Set\n",
        "if os.path.exists(validation_path) and validation_stats:\n",
        "    print(\"\\nüìÇ Preprocessing Validation Set...\")\n",
        "    validation_processed, validation_errors = preprocess_dataset(validation_path, prep_validation)\n",
        "else:\n",
        "    validation_processed, validation_errors = 0, 0\n",
        "    print(\"\\n‚ö†Ô∏è Validation folder not found. Skipping validation preprocessing...\")\n",
        "\n",
        "print(\"\\nüìÇ Preprocessing Testing Set...\")\n",
        "test_processed, test_errors = preprocess_dataset(test_path, prep_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4ms6iiWtlI6"
      },
      "source": [
        "## Summary & Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf_oLF-931lj"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Training images processed: {train_processed:,}\")\n",
        "print(f\"‚ùå Training errors: {train_errors}\")\n",
        "if validation_processed > 0:\n",
        "    print(f\"‚úÖ Validation images processed: {validation_processed:,}\")\n",
        "    print(f\"‚ùå Validation errors: {validation_errors}\")\n",
        "print(f\"‚úÖ Testing images processed: {test_processed:,}\")\n",
        "print(f\"‚ùå Testing errors: {test_errors}\")\n",
        "print(f\"üìÅ Output location: {path}\")\n",
        "\n",
        "# Save summary report\n",
        "if validation_processed > 0:\n",
        "    summary_text = f\"\"\"\n",
        "NeuroScan Preprocessing Report\n",
        "{'='*60}\n",
        "Generated: {pd.Timestamp.now()}\n",
        "\n",
        "Dataset Statistics:\n",
        "- Training images: {sum(train_stats.values()):,}\n",
        "- Validation images: {sum(validation_stats.values()):,}\n",
        "- Testing images: {sum(test_stats.values()):,}\n",
        "- Classes: {len(train_stats)}\n",
        "\n",
        "Preprocessing Results:\n",
        "- Training processed: {train_processed:,}\n",
        "- Training errors: {train_errors}\n",
        "- Validation processed: {validation_processed:,}\n",
        "- Validation errors: {validation_errors}\n",
        "- Testing processed: {test_processed:,}\n",
        "- Testing errors: {test_errors}\n",
        "\n",
        "Output Directories:\n",
        "- Preprocessed Training: {prep_train}\n",
        "- Preprocessed Validation: {prep_validation}\n",
        "- Preprocessed Testing: {prep_test}\n",
        "- Visualizations: {OUTPUT_DIR}\n",
        "\"\"\"\n",
        "else:\n",
        "    summary_text = f\"\"\"\n",
        "NeuroScan Preprocessing Report\n",
        "{'='*60}\n",
        "Generated: {pd.Timestamp.now()}\n",
        "\n",
        "Dataset Statistics:\n",
        "- Training images: {sum(train_stats.values()):,}\n",
        "- Testing images: {sum(test_stats.values()):,}\n",
        "- Classes: {len(train_stats)}\n",
        "\n",
        "Preprocessing Results:\n",
        "- Training processed: {train_processed:,}\n",
        "- Training errors: {train_errors}\n",
        "- Testing processed: {test_processed:,}\n",
        "- Testing errors: {test_errors}\n",
        "\n",
        "Output Directories:\n",
        "- Preprocessed Training: {prep_train}\n",
        "- Preprocessed Testing: {prep_test}\n",
        "- Visualizations: {OUTPUT_DIR}\n",
        "\"\"\"\n",
        "\n",
        "with open(f'{OUTPUT_DIR}/preprocessing_report.txt', 'w') as f:\n",
        "    f.write(summary_text)\n",
        "\n",
        "print(f\"\\nüìÑ Report saved: {OUTPUT_DIR}/preprocessing_report.txt\")\n",
        "print(f\"\\n‚úÖ Phase 2 Complete: Data preprocessing finished\" + (\" (including validation)\" if validation_processed > 0 else \" (train + test only)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA4NBL0iO-Dm"
      },
      "outputs": [],
      "source": [
        "# Move dataset\n",
        "# !mv /root/.cache/kagglehub/datasets/shuvokumarbasakbd/brain-tumors-mri-crystal-clean-colorized-mri-data/versions/1/ /content/NeuroScan/brain-tumors-mri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LhEfwtIjSPG"
      },
      "source": [
        "# Data augmentation and Automated Data loading for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrBjljfOjl7b"
      },
      "source": [
        "### Data Augmentation Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fmQtItdjTUD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA AUGMENTATION & AUTOMATED DATA LOADING\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nCONFIGURING DATA AUGMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use preprocessed data paths\n",
        "train_path = prep_train\n",
        "validation_path = prep_validation\n",
        "test_path = prep_test\n",
        "\n",
        "print(\"Using preprocessed dataset paths:\")\n",
        "print(\"Training:\", train_path)\n",
        "print(\"Testing:\", test_path)\n",
        "\n",
        "# Configure data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    # width_shift_range=0.15,\n",
        "    # height_shift_range=0.15,\n",
        "    # shear_range=0.15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    # fill_mode='nearest',\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    # validation_split=0.15\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    # validation_split=0.15\n",
        ")\n",
        "\n",
        "print(\"Data augmentation configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM0C1oK-j2C_"
      },
      "source": [
        "## Automated data loading pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lJE592fj2_e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSETTING UP AUTOMATED DATA LOADING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "TARGET_SIZE = (224, 224)\n",
        "\n",
        "print(\"Creating data generators...\")\n",
        "\n",
        "# Create data generators with sparse labels for compatibility\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse',\n",
        "    # color_mode='grayscale',\n",
        "    # subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    validation_path,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse',\n",
        "    # subset='validation',\n",
        "    # color_mode='grayscale',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse',\n",
        "    # color_mode='grayscale',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Use more significant names\n",
        "train_dataset = train_generator\n",
        "val_dataset = validation_generator\n",
        "test_dataset = test_generator\n",
        "\n",
        "print(\"Data generators created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgMPFEOo_WhL"
      },
      "source": [
        "## Class Balancing & Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jicdK4Bm_CVy"
      },
      "outputs": [],
      "source": [
        "print(\"\\nANALYZING CLASS DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class_counts = train_generator.classes\n",
        "unique, counts = np.unique(class_counts, return_counts=True)\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(class_counts),\n",
        "    y=class_counts\n",
        ")\n",
        "\n",
        "class_weights_dict = dict(zip(unique, class_weights))\n",
        "\n",
        "print(\"Class Distribution:\")\n",
        "total_samples = sum(counts)\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    count = counts[class_idx]\n",
        "    percentage = (count / total_samples) * 100\n",
        "    weight = class_weights_dict[class_idx]\n",
        "    print(f\"{class_name}: {count} images ({percentage:.1f}%) | Weight: {weight:.3f}\")\n",
        "\n",
        "print(\"Dataset is perfectly balanced\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y4TKydi_Fb_"
      },
      "source": [
        "## Augmentation Visualization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0TNZMeN_J72"
      },
      "outputs": [],
      "source": [
        "print(\"\\nVISUALIZING DATA AUGMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def visualize_augmentations(generator, num_samples=8):\n",
        "    images, labels = next(generator)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = images[i]\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f'Class: {class_names[int(labels[i])]}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('Data Augmentation Examples', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_augmentations(train_generator)\n",
        "print(\"Augmentation visualization completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl-FIlMU_LZJ"
      },
      "source": [
        "## Visualization & Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdvHcac-_L5x"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PIPELINE STATISTICS & SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Pipeline Metrics:\")\n",
        "print(f\"Training Samples: {train_generator.samples}\")\n",
        "print(f\"Validation Samples: {validation_generator.samples}\")\n",
        "print(f\"Test Samples: {test_generator.samples}\")\n",
        "print(f\"Number of Classes: {len(class_names)}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Image Size: {TARGET_SIZE}\")\n",
        "\n",
        "print(\"\\nTask Completed:\")\n",
        "print(\"Data augmentation and automated loading pipeline ready\")\n",
        "\n",
        "train_dataset = train_generator\n",
        "val_dataset = validation_generator\n",
        "test_dataset = test_generator\n",
        "\n",
        "print(\"Using data generators from augmentation pipeline\")\n",
        "print(f\"Training samples: {train_generator.samples}\")\n",
        "print(f\"Validation samples: {validation_generator.samples}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5QG80gyH8vv"
      },
      "source": [
        "# CNN model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MnEdk8PJHvj"
      },
      "source": [
        "## Download and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suL5l5zAJE22"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/addons.git\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow Addons version:\", tfa.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeqi8BYd7JlJ"
      },
      "source": [
        "## Base model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsF4yMq9Ruvv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(256, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')    # ‚Üê 4 classes + softmax\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',   # ‚Üê changed\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXXmT7rEUXW4"
      },
      "source": [
        "### Callbacks definition and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQjpOCSPW9wL"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=5,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_base.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_loss',                # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,               # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='min',                        # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-7, verbose=1)\n",
        "\n",
        "]\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_model_base.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5I2vBbE7rDW"
      },
      "source": [
        "### Training graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56nxBc2i7NCO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8uZwN42XOeJ"
      },
      "source": [
        "### Model metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGGpGJliXRNo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('/content/best_model_base.keras')\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe3nEbx-XRgk"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVOniZBPXVbS"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXtoa6Aw01TY"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0kJogn51IV0"
      },
      "source": [
        "### VGG19 with imagenet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9io5D4f1H7p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√© (VGG19)\n",
        "base_model = VGG19(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "# D√©geler partiellement le mod√®le (ex : les 10 derni√®res couches)\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches personnalis√©es\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)  # Chang√© √† 4 pour vos 4 classes\n",
        "\n",
        "# Cr√©er le mod√®le final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler le mod√®le\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Afficher le r√©sum√© du mod√®le\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7bSi2eS1Vc7"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=3,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_modelVGG19.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',            # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,               # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                        # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_modelVGG19.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6jVlOTu1WMP"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_modelVGG19.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6I6rD6f1eFf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOdozd6e1hmN"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCh--W94mhy"
      },
      "source": [
        "### VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnboBeUL4n0b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√© (VGG19)\n",
        "base_model = VGG16(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "# D√©geler partiellement le mod√®le (ex : les 10 derni√®res couches)\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches personnalis√©es\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)  # Chang√© √† 4 pour vos 4 classes\n",
        "\n",
        "# Cr√©er le mod√®le final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler le mod√®le\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Afficher le r√©sum√© du mod√®le\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6zCGc1G5DQ8"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=3,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_modelVGG16.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',            # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,               # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                        # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_modelVGG16.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmtY25fN5DQ9"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_modelVGG16.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRAK71gW5DQ9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ryDTgYf5DQ9"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtEgeMlA6Lf3"
      },
      "source": [
        "### MobileNet V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eip1Ja4v6TpF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√© (ResNet50)\n",
        "base_model = MobileNetV2(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "# D√©geler partiellement le mod√®le (ex : les 20 derni√®res couches)\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches personnalis√©es\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Cr√©er le mod√®le final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler le mod√®le\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Afficher le r√©sum√© du mod√®le\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cob0ElKl6TpG"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=5,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_modelMobileNetV2.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',               # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,                  # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                           # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_modelMobileNetV2.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQorJqEE6TpG"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_modelMobileNetV2.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0LwzHfi6TpG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0idOpcXl6TpG"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7SvWVKj1iQb"
      },
      "source": [
        "### ResNet50 V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F558fn381nJh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√© (ResNet50)\n",
        "base_model = ResNet50V2(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "# D√©geler partiellement le mod√®le (ex : les 10 derni√®res couches)\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches personnalis√©es\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Cr√©er le mod√®le final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler le mod√®le\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Afficher le r√©sum√© du mod√®le\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bprDURu1pVw"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=5,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_modelResNet50V2.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',               # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,                  # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                           # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_modelResNet50V2.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGtHwPst1sGp"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_modelResNet50V2.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24lxgB7P1y9A"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc_pgSYC10fg"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsYKmm_12IJ"
      },
      "source": [
        "### Resnet 152 V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmyRhCeK19xV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√© (ResNet50)\n",
        "base_model = ResNet152V2(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "# D√©geler partiellement le mod√®le (ex : les 10 derni√®res couches)\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches personnalis√©es\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Cr√©er le mod√®le final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler le mod√®le\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Afficher le r√©sum√© du mod√®le\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpcI3dMb19xV"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=3,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_modelResNet152V2.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',               # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,                  # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                           # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_modelResNet152V2.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRPmivi019xW"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_modelResNet152V2.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2IzLTHv19xW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmmmbRCf19xW"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDrqkmk2tsh"
      },
      "source": [
        "### EfficientNetV2 B0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOU2l7Ku2tds"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input  # IMPORTANT !\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Charger EfficientNetB0\n",
        "base_model = EfficientNetV2B0(\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3),\n",
        "    weights='imagenet',\n",
        "    include_preprocessing=True\n",
        ")\n",
        "\n",
        "# D√©geler davantage de couches (10 derni√®res)\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Ajouter des couches de classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(512, activation='relu')(x)  # Plus de neurones\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Cr√©er le mod√®le\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compiler avec learning rate plus √©lev√©\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHT6Zh8U2zMX"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping : arr√™ter l'entra√Ænement quand la performance ne s'am√©liore plus\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',           # Surveiller la perte de validation\n",
        "        patience=5,                   # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
        "        restore_best_weights=True,    # Restaurer les poids du meilleur mod√®le\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint : sauvegarder le meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_EfficientNetV2B0.keras',  # Nom du fichier de sauvegarde\n",
        "        monitor='val_accuracy',               # Surveiller la pr√©cision de validation\n",
        "        save_best_only=True,                  # Sauvegarder seulement le meilleur mod√®le\n",
        "        mode='max',                           # Mode 'max' car on veut maximiser l'accuracy\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "]\n",
        "# Entra√Æner le mod√®le\n",
        "history = model.fit(\n",
        "    train_dataset,                    # Dataset d'entra√Ænement\n",
        "    epochs=50,                        # Maximum d'√©poques\n",
        "    validation_data=val_dataset,      # Dataset de validation\n",
        "    callbacks=callbacks,              # Callbacks d√©finis ci-dessus\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entra√Ænement termin√©!\")\n",
        "print(f\"Meilleur mod√®le sauvegard√© sous: best_EfficientNetV2B0.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enMYidp621VF"
      },
      "outputs": [],
      "source": [
        "#√âcrivez le code permettant de recharger un mod√®le Keras d√©j√† entra√Æn√© et sauvegard√©.\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('best_EfficientNetV2B0.keras')\n",
        "\n",
        "# Afficher les courbes de loss et d'accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Courbe de loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Courbe d'accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfNkRhmh2_Pz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(test_dataset, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# True labels (correct order)\n",
        "y_true = test_dataset.labels\n",
        "\n",
        "# Class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                            target_names=class_names,\n",
        "                            digits=4))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GLOBAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsshhdbicjrf"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "# Visualization with Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "\n",
        "plt.title('Confusion Matrix - Brain Tumor Classification',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BWT6jC39bIn"
      },
      "source": [
        "# XAI for VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vNvnhOVHop0"
      },
      "source": [
        "## Grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEg7WkQ5osTD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"/content/best_modelVGG16.keras\")\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Preprocessing\n",
        "# ----------------------------\n",
        "def preprocess_image(img_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# ----------------------------\n",
        "# 2. FIXED Grad-CAM\n",
        "# ----------------------------\n",
        "def make_gradcam_heatmap(model, img_array, last_conv_layer_name=\"block5_conv3\"):\n",
        "\n",
        "    # Create Grad-CAM model\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=model.inputs,\n",
        "        outputs=[\n",
        "            model.get_layer(last_conv_layer_name).output,\n",
        "            model.output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        pred_index = tf.argmax(predictions[0])\n",
        "        loss = predictions[:, pred_index]\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "\n",
        "    # Compute channel weights\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Multiply weights √ó feature maps\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.zeros(conv_outputs.shape[:2], dtype=tf.float32)\n",
        "\n",
        "    for i in range(pooled_grads.shape[0]):\n",
        "        heatmap += pooled_grads[i] * conv_outputs[:, :, i]\n",
        "\n",
        "    # Normalize the heatmap\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap) + 1e-8\n",
        "\n",
        "    return heatmap.numpy()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Display Grad-CAM correctly\n",
        "# ----------------------------\n",
        "def display_gradcam(img_path):\n",
        "\n",
        "    # Preprocess for prediction\n",
        "    img_array = preprocess_image(img_path)\n",
        "\n",
        "    # Create heatmap\n",
        "    heatmap = make_gradcam_heatmap(model, img_array)\n",
        "\n",
        "    # Read original image in RGB\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Resize heatmap to image size\n",
        "    heatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Convert heatmap to color\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Superimpose\n",
        "    superimposed = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(superimposed)\n",
        "    plt.title(\"Grad-CAM\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n79gaoDlIaQU"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "display_gradcam(\"/content/NeuroScan/Preprocessed_Training/meningioma_tumor/M_101_DA__7.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXMsEU0Hrvf"
      },
      "source": [
        "## Grad-cam++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgj4Db3g8mZq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import cv2\n",
        "\n",
        "def display_gradcam_plus_plus(model, img_path, last_conv_layer_name=\"block5_conv3\", class_names=None):\n",
        "    \"\"\"\n",
        "    Generates Grad-CAM++ overlay for a single image and displays side by side with the original.\n",
        "\n",
        "    Parameters:\n",
        "    - model: loaded Keras model\n",
        "    - img_path: path to the image\n",
        "    - last_conv_layer_name: name of the last conv layer in the model\n",
        "    - class_names: optional list of class names to display\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load and preprocess image ---\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    img_preprocessed = preprocess_input(img_batch)\n",
        "\n",
        "    # --- Prediction ---\n",
        "    preds = model.predict(img_preprocessed)\n",
        "    predicted_class = np.argmax(preds)\n",
        "    confidence = np.max(preds)\n",
        "    class_label = class_names[predicted_class] if class_names else str(predicted_class)\n",
        "\n",
        "    print(f\"‚úÖ Predicted class: {class_label} | Confidence: {confidence:.4f}\")\n",
        "\n",
        "    # --- Grad-CAM++ function ---\n",
        "    def gradcam_plus_plus(model, img_array, layer_name, class_index=None):\n",
        "        conv_layer = model.get_layer(layer_name)\n",
        "        grad_model = tf.keras.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=[conv_layer.output, model.output]\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            conv_outputs, predictions = grad_model(img_array)\n",
        "            if class_index is None:\n",
        "                class_index = tf.argmax(predictions[0])\n",
        "            loss = predictions[:, class_index]\n",
        "\n",
        "        grads = tape.gradient(loss, conv_outputs)\n",
        "        grads_2 = tf.square(grads)\n",
        "        grads_3 = grads_2 * grads\n",
        "        sum_grads = tf.reduce_sum(conv_outputs * grads_2, axis=(1, 2), keepdims=True)\n",
        "        eps = 1e-10\n",
        "        alpha = grads_3 / (2 * grads_2 + sum_grads + eps)\n",
        "        weights = tf.reduce_sum(alpha * tf.nn.relu(grads), axis=(1, 2))\n",
        "        cam = tf.reduce_sum(weights * conv_outputs, axis=-1)\n",
        "\n",
        "        heatmap = tf.squeeze(cam)\n",
        "        heatmap = tf.maximum(heatmap, 0)\n",
        "        heatmap /= (tf.reduce_max(heatmap) + 1e-10)\n",
        "        return heatmap.numpy()\n",
        "\n",
        "    # --- Generate Grad-CAM++ ---\n",
        "    heatmap = gradcam_plus_plus(model, img_preprocessed, last_conv_layer_name)\n",
        "\n",
        "    # --- Overlay heatmap ---\n",
        "    img_orig = cv2.imread(img_path)\n",
        "    img_orig = cv2.resize(img_orig, (224, 224))\n",
        "    heatmap = cv2.resize(heatmap, (224, 224))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(img_orig, 0.6, heatmap_color, 0.4, 0)\n",
        "\n",
        "    # --- Plot side by side ---\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Input Image\\nClass: {class_label}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Grad-CAM++\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZYtwQ_gKVLl"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/best_modelVGG16.keras'\n",
        "model = load_model(model_path)\n",
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "image_path = '/content/NeuroScan/Preprocessed_Training/meningioma_tumor/M_101_DA__7.jpg'\n",
        "display_gradcam_plus_plus(model, image_path, last_conv_layer_name=\"block5_conv3\", class_names=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RQru9FKHvCw"
      },
      "source": [
        "## Lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKbgWSp_DTBn"
      },
      "outputs": [],
      "source": [
        "!pip install -q lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7z27gexDXyK"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def explain_with_lime_side_by_side(img_array, model, predicted_class, class_names=None, num_samples=1000, top_labels=1):\n",
        "    \"\"\"\n",
        "    Show original image and LIME explanation side by side with class name.\n",
        "\n",
        "    img_array: H x W x C, original image\n",
        "    model: your fine-tuned VGG16\n",
        "    predicted_class: int, predicted class index\n",
        "    class_names: list of class names (optional)\n",
        "    \"\"\"\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def model_predict(imgs):\n",
        "        if imgs.ndim == 3:\n",
        "            imgs = np.expand_dims(imgs, axis=0)\n",
        "        from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "        imgs_preprocessed = preprocess_input(imgs.astype(np.float32))\n",
        "        return model.predict(imgs_preprocessed)\n",
        "\n",
        "    # LIME explanation\n",
        "    explanation = explainer.explain_instance(\n",
        "        img_array.astype('double'),\n",
        "        classifier_fn=model_predict,\n",
        "        top_labels=top_labels,\n",
        "        hide_color=0,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "\n",
        "    temp, mask = explanation.get_image_and_mask(\n",
        "        label=predicted_class,\n",
        "        positive_only=False,\n",
        "        hide_rest=False,\n",
        "        num_features=10,\n",
        "        min_weight=0.01\n",
        "    )\n",
        "\n",
        "    # Build normalized contribution map\n",
        "    feature_weights = dict(explanation.local_exp[predicted_class])\n",
        "    contribution_map = np.zeros(mask.shape, dtype=float)\n",
        "    for feature, weight in feature_weights.items():\n",
        "        contribution_map[mask == feature] = weight\n",
        "\n",
        "    # Normalize to 0-1\n",
        "    min_val, max_val = contribution_map.min(), contribution_map.max()\n",
        "    if max_val - min_val > 0:\n",
        "        contribution_map = (contribution_map - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        contribution_map = np.zeros_like(contribution_map)\n",
        "\n",
        "    # Plot side by side\n",
        "    plt.figure(figsize=(12,6))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(img_array.astype('uint8'))\n",
        "    plt.title(f\"Original Image\\nClass: {class_names[predicted_class] if class_names else predicted_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # LIME explanation overlay\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(mark_boundaries(temp, mask))\n",
        "    plt.imshow(contribution_map, cmap='RdYlBu', alpha=0.5)\n",
        "    plt.title(\"LIME Explanation\")\n",
        "    plt.colorbar(label='Normalized Contribution (0-1)')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xEzY1vsEVzj"
      },
      "outputs": [],
      "source": [
        "class_names = [\"Normal\", \"Glioma Tumor\", \"Meningioma Tumor\", \"Pituitary Tumor\"]\n",
        "\n",
        "image_path = '/content/NeuroScan/Preprocessed_Training/meningioma_tumor/M_497_VF__12.jpg'\n",
        "img = image.load_img(image_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "predicted_class = np.argmax(model.predict(preprocess_input(np.expand_dims(img_array,0))))\n",
        "\n",
        "explain_with_lime_side_by_side(img_array, model, predicted_class, class_names=class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8eh3WISZ81t"
      },
      "source": [
        "# Streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GBkigNk27NN"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRGnk6JwvAMs"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "st.title(\"üß† Brain Tumor Classification (VGG16)\")\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_modelVGG16.keras\")\n",
        "IMG_SIZE = 224\n",
        "CLASSES = [\"glioma\", \"meningioma\", \"normal\", \"pituitary\"]\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload MRI image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "def preprocess(img):\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "if uploaded_file:\n",
        "    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(file_bytes, 1)\n",
        "\n",
        "    st.image(img, channels=\"BGR\", caption=\"Uploaded Image\")\n",
        "\n",
        "    input_img = preprocess(img)\n",
        "    pred = model.predict(input_img)\n",
        "    class_id = np.argmax(pred)\n",
        "\n",
        "    st.success(f\"Prediction: **{CLASSES[class_id].upper()}**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqOZ23vdI4mw"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dlH0kumLE5l"
      },
      "source": [
        "Open logs and copy ip as password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWs49wNPI8CZ"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
